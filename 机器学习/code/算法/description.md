回归 回归 回归
SVR（支持向量机回归）
支持向量机是一种分类算法，但是也可以做回归，
根据输入的数据不同可做不同的模型（若输入标签为连续值则做回归，若输入标签为分类值则用SVC()做分类）。
通过寻求结构化风险最小来提高学习机泛化能力，实现经验风险和置信范围的最小化，从而达到在统计样本量较少的情况下，
亦能获得良好统计规律的目的。通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，
即支持向量机的学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。
对于SVM算法，我们首先导入sklearn.svm中的SVR模块。SVR()就是SVM算法来做回归用的方法（即输入标签是连续值的时候要用的方法），
通过以下语句来确定SVR的模式（选取比较重要的几个参数进行测试。随机选取一只股票开始相关参数选择的测试）。
svr = SVR(kernel=’rbf’, C=1e3, gamma=0.01)

线性回归
线性回归是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。
这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。
线性回归模型经常用最小二乘逼近来拟合，但他们也可能用别的方法来拟合。

多项式回归
将每一维特征的幂次添加为新的特征，再对所有的特征进行线性回归分析。这种方法就是 多项式回归。
当存在多维特征时，多项式回归能够发现特征之间的相互关系，这是因为在添加新特征的时候，添加的是所有特征的排列组合。


分类 分类 分类
KNN（最邻近节点算法）
kNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，
则该样本也属于这个类别，并具有这个类别上样本的特性。
该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。

SVM（支持向量机）
支持向量机是一类按监督学习方式对数据进行二元分类的广义线性分类器，
其决策边界是对学习样本求解的最大边距超平面 。
在数据中找到一个超平面，让尽可能多的数据分布在超平面两侧，距离超平面比较近的点近可能的远离这个超平面

随机森林（random forest）
解决决策树的过拟合问题。
假设训练集包含m个样本，
从m个样本中随机选取一个样本放入采用集中，再把该样本放回到初始数据集中，再次采用。重复m次采用，得到一个包含m个样本的采样集。用这个采样集来训练一个决策树。
决策树划分属性时，先从当前结点的属性集合(假设包含d个属性)中随机选择K个属性，形成一个包含K个属性的集合，再从这k个属性中选出一个最优的属性用于划分。
决策树形成的每个结点都按照步骤2来划分，直到该结点的所有训练样例都是同一类。

logistic
logistic回归是一种简单高效的分类模型，它不仅可以通过学习来预测样本的类别，还可以得到样本属于各个类别的概率信息。因此在机器学习中得到了及其广泛的应用。
logistic分类，即逻辑分类，是一种二分类法，能将数据分为0和1两类，该方法主要由线性求和、sigmoid函数激活、计算误差以及修正参数四个步骤；前两步用于判断，后两步用于修正。


集成方法 集成方法 集成方法
将多个分类方法聚集在一起，以提高分类的准确率。
boosting 
boosting是一个迭代的过程，用于自适应地改变训练样本的分布，使得基分类器聚焦在那些很难分的样本上
boosting会给每个训练样本赋予一个权值，而且可以再每轮提升过程结束时自动地调整权值。开始时，所有的样本都赋予相同的权值1/N，从而使得它们被选作训练的可能性都一样。根据训练样本的抽样分布来抽取样本，得到新的样本集。然后，由该训练集归纳一个分类器，并用它对原数据集中的所有样本进行分类。每轮提升结束时，更新训练集样本的权值。增加被错误分类的样本的权值，减小被正确分类的样本的权值，这使得分类器在随后的迭代中关注那些很难分类的样本。


1.Gradient boosting
Gradient Boosting将负梯度作为上一轮基学习器犯错的衡量指标，在下一轮学习中通过拟合负梯度来纠正上一轮犯的错误。

2.AdaBoost(自适应算法)
AdaBoost是一个具有里程碑意义的算法，是第一个具有适应性的算法，即能适应弱学习器各自的训练误差率。
AdaBoost的具体流程为先对每个样本赋予相同的初始权重，每一轮学习器训练过后都会根据其表现对每个样本的权重进行调整，
增加分错样本的权重，这样先前做错的样本在后续就能得到更多关注，按这样的过程重复训练出M个学习器，最后进行加权组合。
AdaBoostClassifier
使用了两种Adaboost分类算法的实现，SAMME和SAMME.R
AdaBoostRegressor
使用了我们原理篇里讲到的Adaboost回归算法的实现，即Adaboost.R2

voting
投票法（voting）是集成学习里面针对分类问题的一种结合策略。是一种遵循少数服从多数原则的集成学习模型，
通过多个模型的集成降低方差，从而提高模型的 鲁棒性 （算法对数据变化的容忍度有多高）。
在理想情况下，投票法的预测效果应当优于任何一个基模型的预测效果。
1.Voting Regressor
预测结果是所有模型预测结果的平均值。
2.Voting Classifier
预测结果是所有模型种出现最多的预测结果。

BaggingClassifier(装袋)
每个抽样生成的自助样本集上，训练一个基分类器；对训练过的分类器进行投票，将测试样本指派到得票最高的类中。
它在原始训练集的随机子集上建立一个黑箱估计的多个实例，然后将它们各自的预测聚合起来形成最终的预测。
这些方法通过在基本估计量（如决策树）的构造过程中引入随机化，然后对其进行集成，来降低其方差，避免过拟合。

stacking
stacking 就是将一系列模型的输出结果作为新特征输入到其他模型，
这种方法由于实现了模型的层叠，即第一层的模型输出作为第二层模型的输入，
第二层模型的输出作为第三层模型的输入，依次类推，最后一层模型输出的结果作为最终结果。
1.StackingClassifier

2.StackingRegressor


聚类 聚类 聚类
KMeans
K-Means算法的思想：对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。
让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。


降维 降维 降维
PCA
PCA即主成分分析，通过在原始样本变量中选取一些变量来代表样本的大部分信息。
这些选中的变量即为主成分。我们可以调用sklearn中的PCA函数来进行数据主成分分析。
PCA主要是用来数据降维，将高纬度的特征映射到低维度的特征，加快机器学习的速度。